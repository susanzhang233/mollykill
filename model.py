# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M8hgu2P8uL6WWXYaIw5Kr6lPwBwoQQRy

## Encoder & Decoder
"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np
from rdkit import Chem

def featurizer(mol, max_length = 10):
  '''
  Encodes molecules into graph representaions with nodes and edges.
  
  Parameters:
  -------
  mol: rdkit molecule object(rdkit.Chem.rdchem.Mol)
  max_length: max_length of atoms of the molecules to accept

  Returns
  ------
  nodes: an encoded array of atomic numbers of the molecules' atoms
  edges: a matrix indicating the bondtype between each of the atoms within the specified length
  '''
  try:
      from rdkit import Chem
  except ModuleNotFoundError:
      raise ImportError("This function requires RDKit to be installed. To install, refer to here: https://www.rdkit.org/docs/Install.html")

  nodes = []
  edges = []       
 #initiate the encoder of bonds
  bond_types = [
          Chem.rdchem.BondType.ZERO,
          Chem.rdchem.BondType.SINGLE,
          Chem.rdchem.BondType.DOUBLE,
          Chem.rdchem.BondType.TRIPLE,
          Chem.rdchem.BondType.AROMATIC,
        ]
  encoder = {j:i for i, j in enumerate(bond_types,0)} #a dict with keys being rdkit bondtype, values being numbers from 1-5
  
  #loop over the atoms within max_length
  for i in range(max_length):
    #append each atom's corresponding atomic number to the nodes array
    nodes.append(mol.GetAtomWithIdx(i).GetAtomicNum())
    
    l = []
    #loop over the atoms to generate a matrix
    for j in range(max_length):
      #get each of the bonds
      current_bond = mol.GetBondBetweenAtoms(i,j)
    
      if current_bond == None:#some atoms are not connected
        l.append(0)
      else:
        #if connected, encode that bond
        l.append(encoder.get(current_bond.GetBondType()))

    edges.append(l)#append each list to create a bond interaction matrix
     
  return nodes, edges
  #return tf.data.Dataset.from_tensor_slices((
        #{
            #"nodes" : [nodes], 
            #"edges" : [edges]
        #} ) )

##########################################################################

def de_featurizer(nodes, edges):
  '''Draw out a molecule based on the molecules's graph representation with nodes and edges.
      
     Paramenters:
     ------
     nodes: an array of the molecule with atomic numbers
     edges: a matrix containing bond information between each atom of the molecule
     
     Return:
     ------
     two possible rdkit molecules(since the generated molecule graph's edges contains diagonally two possibilities of bond            informations
  '''
 
  mol1 = Chem.RWMol()#initiate two molecules
  mol2 = Chem.RWMol()
  
  bond_types = [
          Chem.rdchem.BondType.ZERO,
          Chem.rdchem.BondType.SINGLE,
          Chem.rdchem.BondType.DOUBLE,
          Chem.rdchem.BondType.TRIPLE,
          Chem.rdchem.BondType.AROMATIC,
        ]
  decoder = {i:j for i, j in enumerate(bond_types,0)}# create decoder of bondtype corresponding with numbers

  #create atoms
  for atom in nodes:
    mol1.AddAtom( Chem.Atom( int(atom)) )
    mol2.AddAtom( Chem.Atom( int(atom)) )
    
  #loop through the matrix to defeaturize bonds
  #mol2 = mol1
  for a in range(len(edges)-1):
    #for b in range(a+1, len(edges)):
    b=a+1
    if 0< edges[int(a)][int(b)] <6:
      mol1.AddBond(int(a),int(b), decoder.get(edges[int(a)][int(b)]))
    else:
      mol1.AddBond(int(a),int(b), Chem.rdchem.BondType.SINGLE)
    
    if 0< edges[int(b)][int(a)] <6:
      mol2.AddBond(int(a),int(b), decoder.get(edges[int(b)][int(a)]))
    else:
      mol2.AddBond(int(a),int(b), Chem.rdchem.BondType.SINGLE)
    


  return mol1, mol2

##########################################################################

"""## Data Preparation"""

#def check_length(min):
  #'''this function checks the length of the molecules and eliminate those that are too short'''
#input_df = ihbt['smiles']
#df_length = []
#for _ in input_df:
  #df_length.append(Chem.MolFromSmiles(_).GetNumAtoms() )
#input_df = input_df.apply(Chem.MolFromSmiles)
#ihbt['length'] = input_df.apply(GetNumAtoms)

"""---------------------

## Discriminator
"""
##########################################################################

def make_discriminator(num_atoms):
  '''
  create a discriminator model that takes in two inputs: nodes and edges of a single molecule
  
  Parameter:
  ------
  num_atoms: the specific length of molecules to consider
  
  Return:
  ------
  A functional keras convolutional 1D graphic neural network model for molcule graphs, with inputs being nodes and edges, output being the possible label
  
  '''
 
  # gnn part for learning edges matrix features
  conv_edge = tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (num_atoms,num_atoms,1))
  edges_tensor = tf.keras.layers.Input(shape = (num_atoms,num_atoms,1), name = 'edges')
  x_edge = conv_edge(edges_tensor)
  
  x_edge = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x_edge)
  x_edge = tf.keras.layers.Flatten()(x_edge)
  x_edge = tf.keras.layers.Dense(64, activation = 'relu')(x_edge)

  # dense layer part for nodes array features
  nodes_tensor = tf.keras.layers.Input(shape = (num_atoms,), name = 'nodes' )
  x_node = tf.keras.layers.Dense(32, activation = 'relu' )(nodes_tensor)
  x_node = tf.keras.layers.Dropout(0.2)(x_node)
  x_node = tf.keras.layers.Dense(64, activation = 'relu')(nodes_tensor)

  main = tf.keras.layers.concatenate([x_node,x_edge], axis = 1)
  main = tf.keras.layers.Dense(32, activation='relu')(main)
  output = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'label')(main)#binary classfication task with sigmoid as the activation
  model = keras.Model(
    inputs = [nodes_tensor, edges_tensor],
    outputs = output
    )
  model.compile(loss='binary_crossentropy', optimizer='adam')
  
  return model

##########################################################################

"""## Generator

"""

def make_generator(num_atoms, noise_input_shape):
  '''
  create a generator model that generates graphic data with noise inputs
  
  Parameter:
  ------
  num_atoms: the specific length of molecules to consider
  noise_input_shape: shape of the noise input to supply to the generative network
  
  Return:
  ------
  A functional keras neural network model for generating molcule graphs of nodes and edges, with inputs being random noise, output being nodes and edges tensors.
  '''
  #start off with some dense layers
  inputs = tf.keras.layers.Input(shape = (noise_input_shape,))
  x = tf.keras.layers.Dense(56, activation="tanh")(inputs)# input_shape = (noise_input_shape,) )#256: filters
 
  x = tf.keras.layers.Dense(56,activation="tanh")(x)

  x = tf.keras.layers.Dense(56,activation="tanh")(x)

  #generating edges
  edges_gen = tf.keras.layers.Dense(units =num_atoms*num_atoms)(x)
  edges_gen = tf.keras.layers.Reshape((num_atoms, num_atoms ))(edges_gen)
   
  #generating nodes
  nodes_gen = tf.keras.layers.Dense(units = num_atoms)(x)
 

  #y = zeros(())
  return keras.Model(
    inputs = inputs,
    outputs = [nodes_gen, edges_gen]
    )

  #return [nodes_gen, edges_gen]
    
##########################################################################

"""## GAN"""

def make_gan(disc, gene, num_atom, noise_input_shape):
  '''
  create a GAN model combining the discriminator with the generator
  
  Parameter:
  ------
  disc: discriminator model to train
  gene: generator model to train
  num_atoms: the specific length of molecules to consider
  noise_input_shape: shape of the noise input to supply to the generative network
  
  Return:
  ------
  A compiled functional keras neural network GAN model for generating molcule graphs of nodes and edges, with inputs being random noise, output being nodes and edges tensors. Upon training, the model would be more likely to generator graphs that are similar yet new to the supplied dataset.
  '''  
  #make discriminator untrainable so that the fake labels would not back propagate the discriminator's accuracy
  disc.trainable = False
    
  ### GAN: input being random noise, combining generator with the discriminator
  inputs = tf.keras.layers.Input(shape = (noise_input_shape,))
  gan = gene(inputs)
  gan = disc(gan)

  model = keras.Model(
    inputs = inputs,
    outputs = gan
    )
  model.compile(loss='binary_crossentropy', optimizer='adam')#, metrics=['mae'])
        
  return model

"""## Training"""

def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):
    """the function plots the loss and accuracy of the model during training process
    
    Parameters:
    --------
    d1_hist: discriminator's loss on the real data(number of average times with a batch that it classifies a real image as fake)
    d2_hist: discriminator's loss with the generated data(number of average times with a batch that it classifies a fake image as real)(desired!)
    g_hist: generator's loss(updated with the combined GAN model via the discriminator's error)
    a1_hist: discriminator's accuracy on the real data
    a2_hist:discriminator's accuracy with the generated data
    """
# plot loss
    plt.subplot(2, 1, 1)
    plt.plot(d1_hist, label='d-real')
    plt.plot(d2_hist, label='d-fake')
    plt.plot(g_hist, label='gen')
    plt.legend()
# plot discriminator accuracy
    plt.subplot(2, 1, 2)
    plt.plot(a1_hist, label='acc-real')
    plt.plot(a2_hist, label='acc-fake')
    plt.legend()
# save plot to file
#pyplot.savefig('results_collapse/plot_line_plot_loss.png')
#plt.close()

def train_batch(disc, gene, nodes, edges, noise_input_shape, EPOCH = 150, BATCHSIZE = 2, plot_hist = True, temp_result = False):
  """
  training function for the GAN model
  
  Parameters:
  ---------
  disc: discriminator model to train
  gene: generator model to train
  
  nodes: nodes training set
  edges: edges training set
  
  noise_input_shape: shape of the noise input to supply to the generative network
  EPOCH: desired training epochs
  BATHSZIE: desired batchsize
  
  plot_hist: wether to plot the summary of loss and accuracy during training
  temp_result: weather to generate some results for observation within interval of epochs during training
  
  Return:
  --------
  gene: A trained generator model
  
  
  """

  #calculate the number of batches per epoch
  batch_per_epoch = int(len(nodes) / BATCHSIZE)
  # number of samples for half a batch
  half_batch = int(BATCHSIZE / 2)
  #length of molecules(in atoms)
  mol_length = len(nodes[0])
  d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(),list(), list(), list(), list()
  #nodes_temp, edges_temp = list(), list()

  #discriminator = make_discriminator(mol_length)
  #disc.compile(optimizer="adam",loss='binary_crossentropy',metrics='accuracy')
  #disc.trainable = False
  #generator = make_generator(mol_length, noise_input_shape)
  #generator.compile(optimizer='adam',loss='binary_crossentropy',metrics='accuracy')
  gan = make_gan(disc, gene, mol_length, noise_input_shape)
  
  i = 0
  #manually step over epochs
  for _ in range(EPOCH):
    #loop real samples for each batch
    N,E = nodes[i:i+half_batch], edges[i:i+half_batch]
    #print(i, i+half_batch)
      #update discriminator model weights with true data
    d_loss1, d_acc1 = disc.train_on_batch([N,E], np.ones((half_batch, 1)) )
      #generate fake samples
    fake_mol_noise = np.random.randint(0, 20,(half_batch,100))
    generated = gene(fake_mol_noise)
    y_fake = np.zeros((half_batch, 1))
    #update discriminator with fake samples(all labels are 0)
    d_loss2, d_acc2 = disc.train_on_batch(generated, y_fake)
    # prepare points in latent space as input for the generator
    X_gan = np.random.randint(0, 80,(BATCHSIZE,100))
    # create inverted labels for the fake samples
    y_gan = np.ones((BATCHSIZE, 1))
    # update the generator via the discriminator's error
    g_loss = gan.train_on_batch(X_gan, y_gan)
    # summarize loss on this batch
    print('>%d, d1=%.3f, d2=%.3f g=%.3f, a1=%d, a2=%d' %
      (i, d_loss1, d_loss2, g_loss, int(100*d_acc1), int(100*d_acc2)))
    # record history
    d1_hist.append(d_loss1)
    d2_hist.append(d_loss2)
    g_hist.append(g_loss)
    a1_hist.append(d_acc1)
    a2_hist.append(d_acc2)
    
    #for an epoch interval of 20, test the generator's performance
    if _%20 ==0:
      if temp_result == True:
        no, ed = gene(np.random.randint(0,20, size =(1,100)))
        m1, m2 = de_featurizer(abs(no.numpy()).astype(int).reshape(num_atoms), abs(ed.numpy()).astype(int).reshape(num_atoms,num_atom))
        print(Chem.MolToSmiles(m1) )
        print(Chem.MolToSmiles(m2) )

    i+=half_batch

  if plot_hist == True:
    plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)


  return gene

"""## Rewarding function"""