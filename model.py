# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M8hgu2P8uL6WWXYaIw5Kr6lPwBwoQQRy

## Encoder & Decoder
"""

def featurizer(mol, max_length = 10):
  '''
  Encodes molecules into 
  Parameters
  -------
  mol: rdkit molecule object(rdkit.Chem.rdchem.Mol)/smiles
  max_length: max_length of atoms of the molecules to accept

  Returns
  ------
  nodes: an encoded array of atomic numbers of the molecules' atoms
  edges: a matrix indicating the bondtype between each of the atoms within the specified length
  '''
  try:
      from rdkit import Chem
  except ModuleNotFoundError:
      raise ImportError("This function requires RDKit to be installed. To install, refer to here: https://www.rdkit.org/docs/Install.html")

  nodes = []
  edges = []       
 #initiate the encoder of bonds
  bond_types = [
          Chem.rdchem.BondType.ZERO,
          Chem.rdchem.BondType.SINGLE,
          Chem.rdchem.BondType.DOUBLE,
          Chem.rdchem.BondType.TRIPLE,
          Chem.rdchem.BondType.AROMATIC,
        ]
  encoder = {j:i for i, j in enumerate(bond_types,1)} #a dict with keys being rdkit bondtype, values being numbers from 1-5
  
  #loop over the atoms within max_length
  for i in range(max_length):
    #append each atom's corresponding atomic number to the nodes array
    nodes.append(mol.GetAtomWithIdx(i).GetAtomicNum())
    
    l = []
    #loop over the atoms to generate a matrix
    for j in range(max_length):
      #get each of the bonds
      current_bond = mol.GetBondBetweenAtoms(i,j)
    
      if current_bond == None:#some atoms are not connected
        l.append(0)
      else:
        #if connected, encode that bond
        l.append(encoder.get(current_bond.GetBondType()))

    edges.append(l)#append each list to create a bond interaction matrix
     
  return nodes, edges
  #return tf.data.Dataset.from_tensor_slices((
        #{
            #"nodes" : [nodes], 
            #"edges" : [edges]
        #} ) )

def de_featurizer(nodes, edges):
  '''draw out a molecule
  '''
 
  mol1 = Chem.RWMol()
  mol2 = Chem.RWMol()
  
  bond_types = [
          Chem.rdchem.BondType.ZERO,
          Chem.rdchem.BondType.SINGLE,
          Chem.rdchem.BondType.DOUBLE,
          Chem.rdchem.BondType.TRIPLE,
          Chem.rdchem.BondType.AROMATIC,
        ]
  decoder = {i:j for i, j in enumerate(bond_types,1)}

  #create atoms
  for atom in nodes:
    mol1.AddAtom( Chem.Atom( int(atom)) )
    mol2.AddAtom( Chem.Atom( int(atom)) )
    
  #defeaturize bonds with the matrix
  #mol2 = mol1
  for a in range(len(edges)-1):
    #for b in range(a+1, len(edges)):
    b=a+1
    if 0< edges[int(a)][int(b)] <6:
      mol1.AddBond(int(a),int(b), decoder.get(edges[int(a)][int(b)]))
    else:
      mol1.AddBond(int(a),int(b), Chem.rdchem.BondType.SINGLE)
    
    if 0< edges[int(b)][int(a)] <6:
      mol2.AddBond(int(a),int(b), decoder.get(edges[int(b)][int(a)]))
    else:
      mol2.AddBond(int(a),int(b), Chem.rdchem.BondType.SINGLE)
    


  return mol1, mol2

"""## Data Preparation"""

#def check_length(min):
  #'''this function checks the length of the molecules and eliminate those that are too short'''
#input_df = ihbt['smiles']
#df_length = []
#for _ in input_df:
  #df_length.append(Chem.MolFromSmiles(_).GetNumAtoms() )
#input_df = input_df.apply(Chem.MolFromSmiles)
#ihbt['length'] = input_df.apply(GetNumAtoms)

"""---------------------

## Discriminator
"""

def make_discriminator(num_atoms):
  '''
  create a discriminator model that takes in two inputs: nodes and edges of a single molecule
  graphic neural network
  '''
  # This is the one!

  #conv_node = layers.Conv2D(
      #32, (3, 3), activation='relu', input_shape=(10, None)
  #)
  conv_edge = layers.Conv1D(32, (3,), activation = 'relu', input_shape = (num_atoms,num_atoms))
  edges_tensor = keras.Input(shape = (num_atoms,num_atoms), name = 'edges')
  x_edge = conv_edge(edges_tensor)
  #x_edge = layers.MaxPooling1D((2,))(x_edge)
  x_edge = layers.Conv1D(64, (3,), activation='relu')(x_edge)
  x_edge = layers.Flatten()(x_edge)
  x_edge = layers.Dense(64, activation = 'relu')(x_edge)

  nodes_tensor = keras.Input(shape = (num_atoms,), name = 'nodes' )
  x_node = layers.Dense(32, activation = 'relu' )(nodes_tensor)
  x_node = layers.Dropout(0.2)(x_node)
  x_node = layers.Dense(64, activation = 'relu')(nodes_tensor)

  main = layers.concatenate([x_node,x_edge], axis = 1)
  main = layers.Dense(32, activation='relu')(main)
  output = layers.Dense(1, activation = 'sigmoid', name = 'label')(main)# number of classes

  return keras.Model(
    inputs = [nodes_tensor, edges_tensor],
    outputs = output
    )

"""## Generator

"""

def make_generator(num_atoms, noise_input_shape):
  '''create generator model
  '''
  inputs = keras.Input(shape = (noise_input_shape,))
  x = layers.Dense(56, activation="tanh")(inputs)# input_shape = (noise_input_shape,) )#256: filters
  #x = layers.Dropout(0.2)(x)
  x = layers.Dense(56,activation="tanh")(x)
  #x = layers.Dropout(0.2)(x)
  x = layers.Dense(56,activation="tanh")(x)

  #generating edges
  edges_gen = layers.Dense(units =num_atoms*num_atoms)(x)
  edges_gen = layers.Reshape((num_atoms, num_atoms ))(edges_gen)

  nodes_gen = layers.Dense(units = num_atoms)(x)
  #assert nodes_gen.output_shape == (num_atoms)
  #nodes_gen = layers.Reshape(num_atoms, num_atoms)(edges_gen)

  #y = zeros(())
  return keras.Model(
    inputs = inputs,
    outputs = [nodes_gen, edges_gen]
    )

  #return [nodes_gen, edges_gen]

"""## GAN"""

def make_gan(disc, gene, num_atom, noise_input_shape):
  #discriminator = make_discriminator(num_atom)
  disc.trainable = False
    # compile discriminator
  #discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002), metrics=['mae'])

    ### generator
    # do not compile generator
  #generator = make_generator(num_atom, noise_input_shape)

    ### GAN 
  inputs = keras.Input(shape = (noise_input_shape,))
  gan = generator(inputs)
  gan = disc(gan)

  model = keras.Model(
    inputs = inputs,
    outputs = gan
    )
  model.compile(loss='binary_crossentropy', optimizer='adam')#, metrics=['mae'])
        
  return model

"""## Training"""

def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):
	# plot loss
	plt.subplot(2, 1, 1)
	plt.plot(d1_hist, label='d-real')
	plt.plot(d2_hist, label='d-fake')
	plt.plot(g_hist, label='gen')
	plt.legend()
	# plot discriminator accuracy
	plt.subplot(2, 1, 2)
	plt.plot(a1_hist, label='acc-real')
	plt.plot(a2_hist, label='acc-fake')
	plt.legend()
	# save plot to file
	#pyplot.savefig('results_collapse/plot_line_plot_loss.png')
	#plt.close()

def train_batch(disc, gene, nodes, edges, noise_input_shape, EPOCH = 2, BATCHSIZE = 128, plot_hist = True, temp_result = False):
  #calculate the number of batches per epoch
  batch_per_epoch = int(len(nodes) / BATCHSIZE)
  # number of samples for half a batch
  half_batch = int(BATCHSIZE / 2)
  #length of molecules(in atoms)
  mol_length = len(nodes[0])
  d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(),list(), list(), list(), list()
  #nodes_temp, edges_temp = list(), list()

  #discriminator = make_discriminator(mol_length)
  disc.compile(optimizer="adam",loss='binary_crossentropy',metrics='accuracy')
  disc.trainable = False
  #generator = make_generator(mol_length, noise_input_shape)
  #generator.compile(optimizer='adam',loss='binary_crossentropy',metrics='accuracy')
  gan = make_gan(disc, gene, mol_length, noise_input_shape)
  
  i = 0
  #manually step over epochs
  for _ in range(EPOCH):
    #loop real samples for each batch
    N,E = nodes[i:i+half_batch], edges[i:i+half_batch]
    #print(i, i+half_batch)
      #update discriminator model weights with true data
    d_loss1, d_acc1 = disc.train_on_batch([N,E], np.ones((half_batch, 1)) )
      #generate fake samples
    fake_mol_noise = np.random.randint(0, 20,(half_batch,100))
    generated = gene(fake_mol_noise)
    y_fake = np.zeros((half_batch, 1))
    #update discriminator with fake samples(all labels are 0)
    d_loss2, d_acc2 = disc.train_on_batch(generated, y_fake)
    # prepare points in latent space as input for the generator
    X_gan = np.random.randint(0, 80,(BATCHSIZE,100))
    # create inverted labels for the fake samples
    y_gan = np.ones((BATCHSIZE, 1))
    # update the generator via the discriminator's error
    g_loss = gan.train_on_batch(X_gan, y_gan)
    # summarize loss on this batch
    print('>%d, d1=%.3f, d2=%.3f g=%.3f, a1=%d, a2=%d' %
      (i, d_loss1, d_loss2, g_loss, int(100*d_acc1), int(100*d_acc2)))
    # record history
    d1_hist.append(d_loss1)
    d2_hist.append(d_loss2)
    g_hist.append(g_loss)
    a1_hist.append(d_acc1)
    a2_hist.append(d_acc2)
    
    if _%20 ==0:
      if temp_result == True:
        no, ed = gene(np.random.randint(0,20, size =(1,100)))
        m1, m2 = de_featurizer(abs(no.numpy()).astype(int).reshape(num_atoms), abs(ed.numpy()).astype(int).reshape(num_atoms,num_atom))
        print(Chem.MolToSmiles(m1) )
        print(Chem.MolToSmiles(m2) )
			#summarize_performance(i, g_model, latent_dim)

    i+=half_batch

  if plot_hist == True:
    plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)


  return gene

"""## Rewarding function"""