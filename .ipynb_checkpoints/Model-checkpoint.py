# -*- coding: utf-8 -*-
"""Model.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-TDbO36D9A5Mea5-y4FFlG17_zjqaAEV

# Setup
To run DeepChem in Colab, we'll first need to run the following lines of code, these will download conda in the colab environment.

# Graphic GAN Model

## Preparations

Now we are ready to import some useful packages.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import deepchem as dc

"""Here we'll import our dataset for training. 
[base_ set](https://pubchem.ncbi.nlm.nih.gov/bioassay/1706)

[dataset](https://github.com/yangkevin2/coronavirus_data)

This dataset contains  In-vitro assay that detects inhibition of SARS-CoV 3CL protease via fluorescence from PubChem AID1706
"""

df = pd.read_csv('https://raw.githubusercontent.com/susanzhang233/mollykill/main/AID1706_binarized_sars.csv')

df.head()

df.groupby('activity').count()

"""Observe the data above, it contains a 'smiles' column, which stands for the smiles representation of the molecules. There is also an 'activity' column, in which it is the label specifying whether that molecule is considered as hit for the protein.

## Train-test split

### Select subpart for testing
"""

from sklearn.model_selection import train_test_split

from sklearn.utils import resample
true = df[df['activity']==1]
false = df[df['activity']==0]

false_short = resample(false, n_samples=2000, replace = False)
ihbt = pd.concat([false_short , true], ignore_index =  True)

"""## Encoder"""

def featurizer(mol, max_length = 10):
  '''
  Parameters
  -------
  mol: rdkit molecule object
  max_length: max_length of molecule to accept

  Returns
  ------
  nodes: an array with atoms encoded by their atomic numbers
  edges: a matrix indicating the bondtype between each of the included atoms
  '''
  nodes = []
  edges = []
  
 #initiate the encoder of bonds
  bond_types = [
          Chem.rdchem.BondType.ZERO,
          Chem.rdchem.BondType.SINGLE,
          Chem.rdchem.BondType.DOUBLE,
          Chem.rdchem.BondType.TRIPLE,
          Chem.rdchem.BondType.AROMATIC,
        ]
  encoder = {j:i for i, j in enumerate(bond_types,1)}
  
  #loop over the atoms within max_length
  for i in range(max_length+1):
    #append each atom's corresponding atomic number to the nodes array
    nodes.append(mol.GetAtomWithIdx(i).GetAtomicNum())
    
    l = []
    #loop over the atoms to generate a matrix
    for j in range(max_length+1):
      #get each of the bonds
      current_bond = mol.GetBondBetweenAtoms(i,j)
    
      if current_bond == None:#some atoms are not connected
        l.append(0)
      else:
        #if connected, encode that bond
        l.append(encoder.get(current_bond.GetBondType()))

    edges.append(l)#append each list to create a bond interaction matrix
     
  return nodes, edges

#def check_size(min):
  #'''this function checks the length of the molecules and eliminate those that are too short'''
input_df = ihbt['smiles']
df_length = []
for _ in input_df:
  df_length.append(Chem.MolFromSmiles(_).GetNumAtoms() )
#input_df = input_df.apply(Chem.MolFromSmiles)
#ihbt['length'] = input_df.apply(GetNumAtoms)

ihbt['length'] = df_length

ihbt = ihbt[ihbt['length']>10]
input_df = ihbt['smiles']
input_df = input_df.apply(Chem.MolFromSmiles)
input_df = input_df.apply(featurizer)

#X_train, X_test, y_train, y_test = train_test_split(input_df, ihbt['activity'], test_size = 0.3)

def de_featurizer(nodes, edges):
  '''draw out a molecule
  '''
  mol = Chem.RWMol()

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""### Real Train Test Split

Since we'll be using multiple inputs, we'll create a tensorflow dataset to handle that for us.
"""

x_train, x_test, y_train, y_test = train_test_split(input_df, ihbt['activity'], test_size = 0.2 )

nodes_train, edges_train = list(zip(*x_train) )
nodes_test, edges_test = list(zip(*x_test))

nd_train = np.array(nodes_train)
eg_train = np.array(edges_train)

nd_test = np.array(nodes_test)
eg_test = np.array(edges_test)

"""-----------------------------

### Train test splits finally!

---------------------

## Discriminator

Now we are finally ready to build a discriminator of our GAN Model. Discriminator has the same logics as a common classification model.
"""

def make_discriminator(num_atoms):
  '''
  create a discriminator model that takes in two inputs: nodes and edges of a single molecule
  graphic neural network
  '''
  # This is the one!

  #conv_node = layers.Conv2D(
      #32, (3, 3), activation='relu', input_shape=(10, None)
  #)
  conv_edge = layers.Conv1D(32, (3,), activation = 'relu', input_shape = (num_atoms,num_atoms))
  edges_tensor = keras.Input(shape = (num_atoms,num_atoms), name = 'edges')
  x_edge = conv_edge(edges_tensor)
  #x_edge = layers.MaxPooling1D((2,))(x_edge)
  x_edge = layers.Conv1D(64, (3,), activation='relu')(x_edge)
  x_edge = layers.Flatten()(x_edge)
  x_edge = layers.Dense(64, activation = 'relu')(x_edge)

  nodes_tensor = keras.Input(shape = (num_atoms,), name = 'nodes' )
  x_node = layers.Dense(32, activation = 'relu' )(nodes_tensor)
  x_node = layers.Dropout(0.2)(x_node)
  x_node = layers.Dense(64, activation = 'relu')(nodes_tensor)

  main = layers.concatenate([x_node,x_edge], axis = 1)
  main = layers.Dense(32, activation='relu')(main)
  output = layers.Dense(1, activation = 'sigmoid', name = 'label')(main)# number of classes

  return keras.Model(
    inputs = [nodes_tensor, edges_tensor],
    outputs = output
    )

discriminator = make_discriminator(11)

"""### Set the weight for training"""

hit_count, nonhit_count = np.bincount(ihbt['activity'])
total_count = len(ihbt['activity'])
weight_nonhit = (1 / nonhit_count) * (total_count) / 2.0
weight_hit = (1 / hit_count) * (total_count) / 2.0
class_weights = {0: weight_nonhit, 1: weight_hit}
#Now, letâ€™s use the weights when training our model:

#model = build_model(X_train, metrics=METRICS)

discriminator.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = discriminator.fit([nd_train, eg_train],
                    y_train,
                    epochs=80, 
                    verbose = False,
                    class_weight=class_weights
                    #steps_per_epoch = 100,
                    )

plt.plot(history.history["accuracy"])
plt.gca().set(xlabel = "epoch", ylabel = "training accuracy")

discriminator.evaluate([nd_test, eg_test], y_test, verbose = 2)

def get_discriminator_loss(real_predictions,fake_predictions):
    real_predictions = tf.sigmoid(real_predictions)#predictions of the real images
    fake_predictions = tf.sigmoid(fake_predictions)#prediction of the images from the generator
    real_loss = tf.losses.binary_crossentropy(tf.ones_like(real_predictions),real_predictions)# as such
    fake_loss = tf.losses.binary_crossentropy(tf.zeros_like(fake_predictions),fake_predictions)
    return real_loss+ fake_loss

nd_test.shape

"""## Generator"""

BATCH_SIZE = len(nd_train)

def make_generator(num_atoms, noise_input_shape):
  '''create generator model
  '''
  inputs = keras.Input(shape = (noise_input_shape,))
  x = layers.Dense(128, activation="tanh")(inputs)# input_shape = (noise_input_shape,) )#256: filters
  #x = layers.Dropout(0.2)(x)
  x = layers.Dense(256,activation="tanh")(x)
  #x = layers.Dropout(0.2)(x)
  x = layers.Dense(528,activation="tanh")(x)

  #generating edges
  edges_gen = layers.Dense(units =num_atoms*num_atoms)(x)
  edges_gen = layers.Reshape((num_atoms, num_atoms ))(edges_gen)

  nodes_gen = layers.Dense(units = num_atoms)(x)
  #assert nodes_gen.output_shape == (num_atoms)
  #nodes_gen = layers.Reshape(num_atoms, num_atoms)(edges_gen)

  #y = zeros(())
  return keras.Model(
    inputs = inputs,
    outputs = [nodes_gen, edges_gen]
    )

  #return [nodes_gen, edges_gen]

generator = make_generator(11, 100)

keras.utils.plot_model(discriminator)

discriminator.predict(generator.predict(np.random.randint(0,2, size =(1,100))))

def get_generator_loss(fake_predictions):
  fake_predictions = tf.sigmoid(fake_predictions)#prediction of the images from the generator
  fake_loss = tf.losses.binary_crossentropy(tf.ones_like(fake_predictions),fake_predictions)
  return fake_loss



"""## GAN"""

#discriminator = make_discriminator(11)
generator = make_generator(11, 100)

def train_step(mol,old_gen_loss,old_disc_loss):
  fake_mol_noise = np.random.randn(batch_size, 100)# input for the generator
  
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:

    real_output = discriminator.evaluate(mol[:2])# trainable = False)

    generated_mols = generator(fake_mol_noise)
    fake_output = discriminator(generated_mols)

    gen_loss = get_generator_loss(fake_output)
    disc_loss = get_discriminator_loss(real_output, fake_output)

        #optimizers would improve the performance of the model with these gradients
    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_disc= disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer = tf.optimizers.Adam(1e-4)
    discriminator_optimizer = tf.optimizers.Adam(1e-4)
    generator_optimizer.apply_gradients(zip(gradients_of_gen, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_disc, discriminator.trainable_variables))

    print('generator loss: ', np.mean(gen_loss))
    print('discriminator loss', np.mean(disc_loss))

def train(nodes,edges, epochs):
  for _ in range(epochs):
    gen_loss = 0
    disc_loss = 0
    for n,e in zip(nodes, edges):
      train = [n.reshape(1,11),e.reshape(1,11,11)]
      train_step(train,gen_loss,disc_loss)
    display.clear_output(wait=True)
    #if (epoch+1)

batch_size = 20
train(nd_train,eg_train, 2)

#batch_size = 20
#train(x_train , 30)

generator(np.random.randint(0,2, size =(1,100)))

discriminator(generator(np.random.randint(0,2, size =(1,100))))

